# æ¨¡å‹è½®æ¢å’Œæ™ºèƒ½æ•…éšœè½¬ç§»ä¿®å¤

## ğŸ› é—®é¢˜æè¿°

ä»ç»ˆç«¯æ—¥å¿—å¯ä»¥çœ‹åˆ°ï¼Œå½“ ModelScope ä¾›åº”å•†é‡åˆ° 429 é”™è¯¯ï¼ˆè¯·æ±‚é™åˆ¶ï¼‰æ—¶ï¼Œç³»ç»Ÿç›´æ¥åˆ‡æ¢åˆ°äº†ä¸‹ä¸€ä¸ªä¾›åº”å•†ï¼ˆOpenAIï¼‰ï¼Œè€Œæ²¡æœ‰å°è¯• ModelScope ä¾›åº”å•†ä¸‹çš„å…¶ä»–æ¨¡å‹ã€‚

**é—®é¢˜æ—¥å¿—**ï¼š
```
2025-10-23 03:45:44,337 - ERROR - OpenAI API error with modelscope: Error code: 429 - {'errors': {'message': 'Request limit exceeded.', 'request_id': 'e1a70c0b-14a9-4cf5-a839-ec196141cf61'}}
2025-10-23 03:45:44,337 - WARNING - âŒ Provider modelscope failed (attempt 1)
ğŸ¯ Using openai with model gpt-4o-mini for claude-haiku-4-5-20251001
ğŸ”„ Switching to fallback provider for claude-haiku-4-5-20251001
```

## ğŸ” æ ¹æœ¬åŸå› 

1. **ç¼ºå°‘æ¨¡å‹ç´¢å¼•è·Ÿè¸ª**: `ProviderState` æ²¡æœ‰è·Ÿè¸ªæ¯ä¸ªä¾›åº”å•†å†…éƒ¨æ¨¡å‹çš„å½“å‰ç´¢å¼•
2. **åªè¿”å›ç¬¬ä¸€ä¸ªæ¨¡å‹**: `get_provider_for_model` å’Œ `get_fallback_provider_for_model` åªè¿”å› `models[0]`
3. **æ²¡æœ‰ä¾›åº”å•†å†…éƒ¨æ•…éšœè½¬ç§»**: å½“æŸä¸ªæ¨¡å‹å¤±è´¥æ—¶ï¼Œæ²¡æœ‰å°è¯•åŒä¸€ä¾›åº”å•†çš„å…¶ä»–æ¨¡å‹

## âœ… ä¿®å¤æ–¹æ¡ˆ

### 1. æ·»åŠ æ¨¡å‹ç´¢å¼•è·Ÿè¸ª

åœ¨ `ProviderState` ä¸­æ·»åŠ äº†æ¨¡å‹è½®æ¢è·Ÿè¸ªï¼š

```python
@dataclass
class ProviderState:
    # ... å…¶ä»–å­—æ®µ ...
    # Model rotation tracking
    model_indices: Dict[str, int] = None  # Track current model index for each type
    
    def __post_init__(self):
        if self.model_indices is None:
            self.model_indices = {}
    
    def get_next_model(self, model_type: str) -> Optional[str]:
        """Get next model for the given type, cycling through available models"""
        models = getattr(self.provider.models, model_type, [])
        if not models:
            return None
        
        # Get current index for this model type
        current_index = self.model_indices.get(model_type, 0)
        
        # Get the model at current index
        model = models[current_index]
        
        # Update index for next time (cycle through models)
        self.model_indices[model_type] = (current_index + 1) % len(models)
        
        return model
```

### 2. æ›´æ–°ä¾›åº”å•†é€‰æ‹©é€»è¾‘

ä¿®æ”¹äº† `get_provider_for_model` å’Œ `get_fallback_provider_for_model` æ–¹æ³•ï¼š

```python
def get_provider_for_model(self, model_type: str) -> Optional[Tuple[ProviderState, str]]:
    """Get provider and model for the given model type (big/middle/small)"""
    available_providers = self.get_available_providers()
    
    if not available_providers:
        return None
    
    # Try each provider in priority order
    for state in available_providers:
        model = state.get_next_model(model_type)  # ä½¿ç”¨è½®æ¢æœºåˆ¶
        if model:
            return state, model
    
    return None
```

### 3. æ·»åŠ ä¾›åº”å•†å†…éƒ¨æ¨¡å‹åˆ‡æ¢

æ·»åŠ äº† `get_next_model_for_provider` æ–¹æ³•ï¼š

```python
def get_next_model_for_provider(self, provider_name: str, model_type: str) -> Optional[Tuple[ProviderState, str]]:
    """Get next model for a specific provider, cycling through its models"""
    if provider_name not in self.providers:
        return None
    
    state = self.providers[provider_name]
    if state.status != ProviderStatus.HEALTHY:
        return None
    
    model = state.get_next_model(model_type)
    if model:
        return state, model
    
    return None
```

### 4. å®ç°æ™ºèƒ½æ•…éšœè½¬ç§»é€»è¾‘

åœ¨ `model_manager.py` ä¸­æ·»åŠ äº† `get_next_model_for_provider` æ–¹æ³•ï¼š

```python
async def get_next_model_for_provider(self, claude_model: str, provider_name: str) -> Optional[Tuple[any, str, str]]:
    """Get next model for a specific provider, cycling through its models"""
    # ... å®ç°é€»è¾‘ ...
```

### 5. æ›´æ–°APIç«¯ç‚¹æ•…éšœè½¬ç§»

åœ¨ `endpoints.py` ä¸­å®ç°äº†æ™ºèƒ½æ•…éšœè½¬ç§»é€»è¾‘ï¼š

```python
async def try_fallback(request: ClaudeMessagesRequest, http_request: Request, _, failed_provider: str, error: HTTPException):
    """Try fallback logic: first try other models in same provider, then other providers"""
    logger.error(f"Request failed with {failed_provider}: {error.detail}")
    
    # First, try other models in the same provider
    if config.provider_manager:
        next_model_result = await model_manager.get_next_model_for_provider(request.model, failed_provider)
        if next_model_result:
            colored_logger.warning(f"ğŸ”„ Trying next model in {failed_provider}")
            # ... å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹ ...
        
        # If no more models in same provider, try other providers
        config.provider_manager.mark_provider_failure(failed_provider, error)
        fallback_result = await model_manager.get_client_and_model(request.model, failed_provider)
        if fallback_result:
            colored_logger.warning(f"ğŸ”„ Switching to fallback provider for {request.model}")
            return await handle_request_with_fallback(request, http_request, _)
```

## ğŸ¯ ä¿®å¤æ•ˆæœ

### æ¨¡å‹è½®æ¢æµ‹è¯•

æµ‹è¯•ç»“æœæ˜¾ç¤ºæ¨¡å‹è½®æ¢æ­£å¸¸å·¥ä½œï¼š

```
Call 1: modelscope:Qwen/Qwen2.5-7B-Instruct
Call 2: modelscope:Qwen/Qwen3-8B  
Call 3: modelscope:Qwen/Qwen3-32B
Call 4: modelscope:Qwen/Qwen2.5-7B-Instruct  # å¾ªç¯å›åˆ°ç¬¬ä¸€ä¸ª
Call 5: modelscope:Qwen/Qwen3-8B
```

### æ•…éšœè½¬ç§»ç­–ç•¥

ç°åœ¨çš„æ•…éšœè½¬ç§»ç­–ç•¥æ˜¯ï¼š

1. **ä¾›åº”å•†å†…éƒ¨æ•…éšœè½¬ç§»**: å½“æŸä¸ªæ¨¡å‹å¤±è´¥æ—¶ï¼Œå…ˆå°è¯•åŒä¸€ä¾›åº”å•†çš„å…¶ä»–æ¨¡å‹
2. **ä¾›åº”å•†é—´æ•…éšœè½¬ç§»**: å½“ä¾›åº”å•†å†…æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥æ—¶ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªä¾›åº”å•†
3. **æ¨¡å‹è½®æ¢**: æ¯æ¬¡è¯·æ±‚éƒ½ä¼šè½®æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹

## ğŸ“Š é…ç½®ç¤ºä¾‹

ä»¥ ModelScope ä¾›åº”å•†ä¸ºä¾‹ï¼Œé…ç½®äº†å¤šä¸ª small æ¨¡å‹ï¼š

```json
{
  "name": "modelscope",
  "enabled": true,
  "priority": 1,
  "api_key": "${MODELSCOPE_API_KEY}",
  "base_url": "https://api-inference.modelscope.cn/v1/",
  "models": {
    "small": ["Qwen/Qwen2.5-7B-Instruct", "Qwen/Qwen3-8B", "Qwen/Qwen3-32B"]
  }
}
```

ç°åœ¨å½“ `Qwen/Qwen2.5-7B-Instruct` é‡åˆ° 429 é”™è¯¯æ—¶ï¼Œç³»ç»Ÿä¼šï¼š

1. å°è¯• `Qwen/Qwen3-8B`
2. å¦‚æœä¹Ÿå¤±è´¥ï¼Œå°è¯• `Qwen/Qwen3-32B`
3. å¦‚æœæ‰€æœ‰ ModelScope æ¨¡å‹éƒ½å¤±è´¥ï¼Œæ‰åˆ‡æ¢åˆ° OpenAI ä¾›åº”å•†

## ğŸ‰ æ€»ç»“

ä¿®å¤åçš„ç³»ç»Ÿç°åœ¨æ”¯æŒï¼š

1. **æ™ºèƒ½æ¨¡å‹è½®æ¢**: æ¯æ¬¡è¯·æ±‚è‡ªåŠ¨è½®æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹
2. **ä¾›åº”å•†å†…éƒ¨æ•…éšœè½¬ç§»**: æ¨¡å‹å¤±è´¥æ—¶å…ˆå°è¯•åŒä¸€ä¾›åº”å•†çš„å…¶ä»–æ¨¡å‹
3. **ä¾›åº”å•†é—´æ•…éšœè½¬ç§»**: ä¾›åº”å•†å†…æ‰€æœ‰æ¨¡å‹å¤±è´¥æ—¶æ‰åˆ‡æ¢ä¾›åº”å•†
4. **æ›´å¥½çš„èµ„æºåˆ©ç”¨**: å……åˆ†åˆ©ç”¨æ¯ä¸ªä¾›åº”å•†çš„å¤šä¸ªæ¨¡å‹

è¿™å¤§å¤§æé«˜äº†ç³»ç»Ÿçš„å¯é æ€§å’Œèµ„æºåˆ©ç”¨ç‡ï¼

---

**ä¿®å¤æ—¶é—´**: 2025-10-23  
**çŠ¶æ€**: âœ… å®Œæˆå¹¶æµ‹è¯•é€šè¿‡


